{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f7497c3",
   "metadata": {},
   "source": [
    "<h1 align=\"center\"> Content Based Image Retrieval\n",
    " </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9f94cf",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{CSE2230 Multimedia Analysis}\\\\\n",
    "\\text{2021-2022 Q3 Week 2}\\\\\n",
    "\\textbf{Deadline:}\\text{ 22 Feburary 2022}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0019269e",
   "metadata": {},
   "source": [
    "### How to submit your work:\n",
    "After making this jupiter notebook, in brightspace you can find under Labs -> Lab 2 -> YOURNAME_LAB1.docx. Fill in this document and convert it to a PDF or Docx file. In brightspace under assigments there will be a assigment called **\"Lab 2\"**. Here you can hand in the PDF or Docx you just created with your answers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8527340f",
   "metadata": {},
   "source": [
    "### Prerquisites:\n",
    "During today's lab, you will be using an extra set of images. Before getting started, check if there is a folder called `studentimages` in the same folder as this notebook. If not, make the folder `studentimages`. After you have done this download the images from here: https://drive.google.com/drive/folders/1tvvere7UX07dpWJY0SXbnf0Fv6HOZmlZ?usp=sharing and put them in this folder. You will notice that the images are named using the following annotations:\n",
    "\n",
    "    y_tag.jpg\n",
    "\n",
    "- y = ewi for “Photos of EWI”\n",
    "- y = d for the “Direction photos”\n",
    "- y = b|w|l  for the “Delft Campus Visitor photos” (according to whether it’s a photo of the bike, water, or streetlight)\n",
    "- tag = a photo tag, which is different for the different types of photos (a descriptive adjective) for “Direction photos”: tag=N|S|E|W|NE|SE|NW|SW\n",
    "\n",
    "After this do the same for the `Image/joint` folder from this link: https://drive.google.com/drive/folders/1IDm27SqMYBuqBGWRB-aI0UDRxgAzkf9O. Here you also find the databese we are going to use. These databases should be in the `Databases` folder. \n",
    "\n",
    "### Our goals today:\n",
    "#### Part I: OpenCV Continued\n",
    "After this lab, you should be comfortable with OpenCV. You should also have gained (more) practical experience with SIFT keypoints and descriptors. It is important to refresh SIFT descriptors so that you understand how SIFT descriptors are used to create visual words in Part II of today’s lab.\n",
    "\n",
    "#### Part II: Content Based Image Retrieval\n",
    "After this lab, you will be able to build your own content-based image retrieval system using color histograms and visual words. You should understand how CBIR can be used to find near-duplicate matches and semantic matches. You also should be able to discuss the difference between the algorithm perspective and the user perspective on CBIR.\n",
    "\n",
    "By using the given images of Delft, you gain a hands-on understanding of the sensitivity of image analysis and retrieval to the wide variability of real-world images. \n",
    "\n",
    "Multimedia research aims to close the semantic gap. The semantic gap is defined as, “the difference between the information that a machine can extract from the (perceptual) data and the interpretation that a user in a given situation has for the same data.” We want to comprehend the full implications of the semantic gap."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496fa5e5",
   "metadata": {},
   "source": [
    "### 1 OpenCV Continued \n",
    "In this part of today’s lab, we will be using SIFT to match near-duplicate images. This section helps you to review SIFT and also gives you more experience with OpenCV.\n",
    "\n",
    "For a small summary of SIFT with OpenCV visit: \n",
    "https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_feature2d/py_sift_intro/py_sift_intro.html\n",
    "\n",
    "### 1.1 SIFT function in OpenCV\n",
    "Tip: remember to activate your virtual environment before working with Python.\n",
    "If you open Python in your shell window you can use the following code to read the documentation on SIFT:\n",
    "\n",
    "    import cv2\n",
    "    help(cv2.xfeatures2d.SIFT_create())\n",
    "\n",
    "The command-line documentation is a bit dry and more useful if you already know a\n",
    "bit about the functionality. Online sources are typically better for more descriptive\n",
    "information. Note that in a lot of online documentation you will find cv2.SIFT instead of cv2.xfeatures2d.SIFT create() . This is because the latter notation has only been in use since OpenCV 3.0.\n",
    "\n",
    "If you get the error that xfeatures2d does not exist, open the anaconda prompt and type the following:\n",
    "    `pip3 install numpy opencv-python==3.4.2.16 opencv-contrib-python==3.4.2.16`\n",
    "This should install the right opencv functions for Python 3.0 that we use during this course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75efe080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on xfeatures2d_SIFT object:\n",
      "\n",
      "class xfeatures2d_SIFT(Feature2D)\n",
      " |  Method resolution order:\n",
      " |      xfeatures2d_SIFT\n",
      " |      Feature2D\n",
      " |      Algorithm\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __new__(*args, **kwargs) from builtins.type\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      " |  \n",
      " |  __repr__(self, /)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  create(...) from builtins.type\n",
      " |      create([, nfeatures[, nOctaveLayers[, contrastThreshold[, edgeThreshold[, sigma]]]]]) -> retval\n",
      " |      .   @param nfeatures The number of best features to retain. The features are ranked by their scores\n",
      " |      .   (measured in SIFT algorithm as the local contrast)\n",
      " |      .   \n",
      " |      .   @param nOctaveLayers The number of layers in each octave. 3 is the value used in D. Lowe paper. The\n",
      " |      .   number of octaves is computed automatically from the image resolution.\n",
      " |      .   \n",
      " |      .   @param contrastThreshold The contrast threshold used to filter out weak features in semi-uniform\n",
      " |      .   (low-contrast) regions. The larger the threshold, the less features are produced by the detector.\n",
      " |      .   \n",
      " |      .   @param edgeThreshold The threshold used to filter out edge-like features. Note that the its meaning\n",
      " |      .   is different from the contrastThreshold, i.e. the larger the edgeThreshold, the less features are\n",
      " |      .   filtered out (more features are retained).\n",
      " |      .   \n",
      " |      .   @param sigma The sigma of the Gaussian applied to the input image at the octave \\#0. If your image\n",
      " |      .   is captured with a weak camera with soft lenses, you might want to reduce the number.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from Feature2D:\n",
      " |  \n",
      " |  compute(...)\n",
      " |      compute(image, keypoints[, descriptors]) -> keypoints, descriptors\n",
      " |      .   @brief Computes the descriptors for a set of keypoints detected in an image (first variant) or image set\n",
      " |      .   (second variant).\n",
      " |      .   \n",
      " |      .   @param image Image.\n",
      " |      .   @param keypoints Input collection of keypoints. Keypoints for which a descriptor cannot be\n",
      " |      .   computed are removed. Sometimes new keypoints can be added, for example: SIFT duplicates keypoint\n",
      " |      .   with several dominant orientations (for each orientation).\n",
      " |      .   @param descriptors Computed descriptors. In the second variant of the method descriptors[i] are\n",
      " |      .   descriptors computed for a keypoints[i]. Row j is the keypoints (or keypoints[i]) is the\n",
      " |      .   descriptor for keypoint j-th keypoint.\n",
      " |      \n",
      " |      \n",
      " |      \n",
      " |      compute(images, keypoints[, descriptors]) -> keypoints, descriptors\n",
      " |      .   @overload\n",
      " |      .   \n",
      " |      .   @param images Image set.\n",
      " |      .   @param keypoints Input collection of keypoints. Keypoints for which a descriptor cannot be\n",
      " |      .   computed are removed. Sometimes new keypoints can be added, for example: SIFT duplicates keypoint\n",
      " |      .   with several dominant orientations (for each orientation).\n",
      " |      .   @param descriptors Computed descriptors. In the second variant of the method descriptors[i] are\n",
      " |      .   descriptors computed for a keypoints[i]. Row j is the keypoints (or keypoints[i]) is the\n",
      " |      .   descriptor for keypoint j-th keypoint.\n",
      " |  \n",
      " |  defaultNorm(...)\n",
      " |      defaultNorm() -> retval\n",
      " |      .\n",
      " |  \n",
      " |  descriptorSize(...)\n",
      " |      descriptorSize() -> retval\n",
      " |      .\n",
      " |  \n",
      " |  descriptorType(...)\n",
      " |      descriptorType() -> retval\n",
      " |      .\n",
      " |  \n",
      " |  detect(...)\n",
      " |      detect(image[, mask]) -> keypoints\n",
      " |      .   @brief Detects keypoints in an image (first variant) or image set (second variant).\n",
      " |      .   \n",
      " |      .   @param image Image.\n",
      " |      .   @param keypoints The detected keypoints. In the second variant of the method keypoints[i] is a set\n",
      " |      .   of keypoints detected in images[i] .\n",
      " |      .   @param mask Mask specifying where to look for keypoints (optional). It must be a 8-bit integer\n",
      " |      .   matrix with non-zero values in the region of interest.\n",
      " |      \n",
      " |      \n",
      " |      \n",
      " |      detect(images[, masks]) -> keypoints\n",
      " |      .   @overload\n",
      " |      .   @param images Image set.\n",
      " |      .   @param keypoints The detected keypoints. In the second variant of the method keypoints[i] is a set\n",
      " |      .   of keypoints detected in images[i] .\n",
      " |      .   @param masks Masks for each input image specifying where to look for keypoints (optional).\n",
      " |      .   masks[i] is a mask for images[i].\n",
      " |  \n",
      " |  detectAndCompute(...)\n",
      " |      detectAndCompute(image, mask[, descriptors[, useProvidedKeypoints]]) -> keypoints, descriptors\n",
      " |      .   Detects keypoints and computes the descriptors\n",
      " |  \n",
      " |  empty(...)\n",
      " |      empty() -> retval\n",
      " |      .\n",
      " |  \n",
      " |  getDefaultName(...)\n",
      " |      getDefaultName() -> retval\n",
      " |      .\n",
      " |  \n",
      " |  read(...)\n",
      " |      read(fileName) -> None\n",
      " |      .   \n",
      " |      \n",
      " |      \n",
      " |      \n",
      " |      read(arg1) -> None\n",
      " |      .\n",
      " |  \n",
      " |  write(...)\n",
      " |      write(fileName) -> None\n",
      " |      .   \n",
      " |      \n",
      " |      \n",
      " |      \n",
      " |      write(fs[, name]) -> None\n",
      " |      .\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from Algorithm:\n",
      " |  \n",
      " |  clear(...)\n",
      " |      clear() -> None\n",
      " |      .   @brief Clears the algorithm state\n",
      " |  \n",
      " |  save(...)\n",
      " |      save(filename) -> None\n",
      " |      .   Saves the algorithm to a file.\n",
      " |      .   In order to make this method work, the derived class must implement Algorithm::write(FileStorage& fs).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "help(cv2.xfeatures2d.SIFT_create())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a3f26e",
   "metadata": {},
   "source": [
    "### 1.2 Testing SIFT module\n",
    "Previous steps have provided us with some information about the function parameters. Calling this function returns an `xfeatures2d_SIFT` object. The square brackets around parameters indicate that the parameters are optional. This means we can either just call `cv2.xfeatures2d.SIFT_create()` in which case the optional parameters are assigned default values, or we can assign them with specific values like this:\n",
    "\n",
    "    sift = cv2.xfeatures2d.SIFT_create(nfeatures = 10, sigma=3)\n",
    "\n",
    "The help text provided by OpenCV does not explain the type of the parameters and what the different parameters are used for. To learn more about the OpenCV functions we have to consult the online documentation.\n",
    "\n",
    "#### ***Answer the following question:***\n",
    "Find the online documentation and explain the parameters nfeatures and sigma of\n",
    "the SIFT function shown above in your own words. Note: some parts of the OpenCV\n",
    "documentation are only available for C++, however the parameters of the functions\n",
    "are similar.\n",
    "\n",
    "#### Write your answer here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51f1d93",
   "metadata": {},
   "source": [
    "<font color='red'>\n",
    "    ANSWER\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4456a4b",
   "metadata": {},
   "source": [
    "### 1.3 Extracting SIFT keypoints\n",
    "With our newfound knowledge about the `cv2.xfeatures2d.SIFT_create()` function we can put it to work. However, this function is just a constructor. Besides returning a SIFT object, it does nothing. To do the actual SIFT transform we first have to feed our target image to the SIFT object to obtain a list of keypoints.\n",
    "\n",
    "First, create the SIFT object and load an image.\n",
    "\n",
    "    sift = cv2.xfeatures2d.SIFT_create()\n",
    "    im = cv2.imread('path/to/image.jp', cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "$\\textbf{Tip:}$ make sure the path to the image is correct. Remember the path is relative to your working directory (ie. where you run the code from).\n",
    "\n",
    "Figure out how to compute the SIFT keypoints and add it to your code. Search for a function that outputs the keypoints (there is helpful information for this online). Note: you will need the SIFT keypoints in the following exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5751ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_keypoints(img):\n",
    "    keypoints = None\n",
    "    # Start answer here\n",
    "    \n",
    "    # End snswer here\n",
    "    return keypoints\n",
    "\n",
    "bookshelf_image = None\n",
    "bookshelf_keypoints = None\n",
    "sift = None\n",
    "# Start answer here\n",
    "    \n",
    "# End snswer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11308e2b",
   "metadata": {},
   "source": [
    "### 1.4 Displaying keypoints\n",
    "OpenCV has a nice helper function that draws the keypoints for you. It is called as follows:\n",
    "\n",
    "    k_im = cv2.drawKeypoints(im, keypoints, None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "\n",
    "Now plot the image like this:\n",
    "\n",
    "    import matplotlib.cm as cm\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.imshow(k_im, cmap=cm.Greys_r)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ec074b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start answer here\n",
    "    \n",
    "# End snswer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3812c83",
   "metadata": {},
   "source": [
    "#### ***Answer the following questions:***\n",
    "\n",
    "1. What do the centre points of the circles correspond to?\n",
    "2. And the line inside the circles?\n",
    "3. And the size of the circles?\n",
    "4. In what kind of image regions (homogeneous, edge, corner, etc) does the SIFT detector find keypoints?\n",
    "5. What happens at the borders of the image?\n",
    "\n",
    "#### Write your answers here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1506a3",
   "metadata": {},
   "source": [
    "<font color='red'>\n",
    "    ANSWER\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d3a32c",
   "metadata": {},
   "source": [
    "### 1.5  SIFT Descriptors \n",
    "We now would like to compare different images based on the SIFT descriptors. \n",
    "\n",
    "We can compute the SIFT keypoints and descriptors using:\n",
    "\n",
    "    kp, desc = sift.detectAndCompute(im, None)\n",
    "\n",
    "Here is an example descriptor for a single keypoint:\n",
    "\n",
    "    [ 114.   16.    1.    0.    0.    2.   39.  114.    8.    7.    3.    0.\n",
    "     1.   72.  114.   47.    5.    2.    0.    0.    0.   38.   91.   19.\n",
    "    15.   11.    1.    0.    0.    7.   26.    9.  114.  114.   17.    0.\n",
    "     0.    1.   15.   62.  114.  107.   29.    5.    4.   46.   61.   52.\n",
    "    14.    6.    9.   32.   56.  114.   95.   23.    2.    2.    1.    3.\n",
    "     7.   23.   69.   17.   83.   23.    1.    0.    0.    1.   47.   93.\n",
    "     114.   95.   73.   19.    6.   15.   33.   35.   10.   28.  114.  114.\n",
    "    36.   16.    8.    6.   15.   71.   30.    8.    4.    3.   20.   13.\n",
    "    77.   50.   10.    0.    0.    2.   11.   35.   11.   30.   60.   18.\n",
    "    13.   26.   17.    6.   11.    7.   24.    8.    4.   10.   30.   22.\n",
    "     6.   29.    7.    0.    0.    4.   48.   21.]\n",
    "\n",
    "It is a vector with 128 dimensions. Do you remember what the individual components of this vector represent? (Not necessary to write the answer here. Just make sure that you remember.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192694ff",
   "metadata": {},
   "source": [
    "### 1.6 SIFT Matching \n",
    "Next, we will match two images using the SIFT keypoints and their descriptors. Find the images nieuwekerk1.jpg and nieuwekerk2.jpg or notreDame1.jpg and notreDame2.jpg in the `Images` folder. \n",
    "\n",
    "Attempt to show the matching between them using the following code:\n",
    "\n",
    "    # create BFMatcher object\n",
    "    bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)\n",
    "\n",
    "    # Match descriptors.\n",
    "    matches = bf.match(des1,des2)\n",
    "\n",
    "    # Sort them in the order of their distance.\n",
    "    matches = sorted(matches, key = lambda x:x.distance)\n",
    "\n",
    "    # Draw first 10 matches.\n",
    "    img3 = cv2.drawMatches(img1,kp1,img2,kp2,matches[:10], None, flags=2)\n",
    "\n",
    "    plt.imshow(img3)\n",
    "    plt.show()\n",
    "\n",
    "Source:\n",
    "https://docs.opencv.org/3.4/dc/dc3/tutorial_py_matcher.html\n",
    "\n",
    "Your result should resemble:\n",
    "![Example](Images/Example1-6.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2908733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start answer here\n",
    "    \n",
    "# End snswer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4154c23e",
   "metadata": {},
   "source": [
    "#### ***Answer the following questions:***\n",
    "\n",
    "1. Do the points in the photos connected by the match lines correspond to the same physical point in the real-world scene depicted by the photo (i.e., to the same part of the building)? \n",
    "2. Do they correspond to points that have the same shape on the real-world building?\n",
    "3. What do you expect happens when you rotate one of the images by 90 degrees? Are the same physical points in the real-world scene matched?\n",
    "4. Now, create below another version of this image pair that depicts the next 10 best matches, i.e., `[10:20]`.\n",
    "5. Do you notice a difference between the top 10 best matches and the next 10 best matches in terms of their ability to connect points in both photos corresponding to the same real-world point? Did you observe what you expected to observe? Try pushing the limit further, what would be a good cutoff to use? \n",
    "\n",
    "\n",
    "#### Write your answers here (except 4):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71c1551",
   "metadata": {},
   "source": [
    "<font color='red'>\n",
    "    ANSWER\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bb55e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For question 4 from 1.6\n",
    "# Start answer here\n",
    "    \n",
    "# End snswer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56e4ad0",
   "metadata": {},
   "source": [
    "## 2 CBIR\n",
    "CBIR stands for Content-Based Image Retrieval. CBIR techniques retrieve images based on their content (pixels). In this lab, we look at Query-by-image, which means that the query is itself an image. \n",
    "\n",
    "When studying CBIR it is important to differentiate the Algorithm perspective from the User perspective.\n",
    "\n",
    "1. **From the Algorithm perspective:** Images are represented by features derived from pixels. CBIR compares images on the basis of these features. Standardly, a feature vector is built for the query image, and is compared to the feature vectors of each image in the collection. We distinguish local features, which represent certain image regions, and global features, which represent the image as a whole. Visual words built from SIFT descriptors are an example of local features. Features based on the color histogram are an example of global features. Good visual features are good at capturing the sorts of similarities between images that human users see.\n",
    "\n",
    "2. **From the User perspective:** The goal of the CBIR system is to return images that are relevant to the user information need behind the query. Users with different information needs might submit the same query to an information retrieval system. For example, in the case of images depicting an object, a user might want to find images depicting the same object instance, or the same object class. The system should try to satisfy the users as well as possible.\n",
    "\n",
    "To carry out a formal evaluation of a CBIR system it is necessary to have ground truth that specifies which images are relevant to which queries. There are two main ways of creating ground truth: first, recording it at the time at which the image is captured (for example, using the camera GPS or adding a tag) and, second, creating it by having a large number of people look at images and judge whether or not they are relevant to a specific information need. Which method you use depends on your application, and also on the resources available to you (whether you have a large number of people willing to judge images for you). True evaluation requires a test set consisting of a list of queries. For each query, you test the system and record how well the system scores with respect to a specific metric (e.g., precision or recall). The final score is usually reported as the average score overall queries. Remember in this lab you are not carrying out a formal evaluation, but rather trying to get an informal feel for how the system works.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a23d7e6",
   "metadata": {},
   "source": [
    "### 2.1 Representing images as vectors\n",
    "\n",
    "In order to carry out Content Based Image Retrieval, we need to be able to compare images using feature vectors. \n",
    "\n",
    "First, let’s consider the case of the color histogram. The color histogram has three series of 256 bins, which we can easily convert to a single vector with normalized bin heights. At that point, we can pick and choose from a plethora of similarity measures.\n",
    "\n",
    "In contrast, if we consider SIFT descriptors, it is not immediately obvious how we should proceed. One image might have more SIFT descriptors than the next image. Even assuming that we can magically enforce how many SIFT descriptors there are, it is still not clear how to calculate the similarity between two different sets of SIFT descriptors. Fortunately, however, there is another technique for approaching these issues.\n",
    "\n",
    "In the lectures, we have discussed the Bag of Words approach. Here, we apply the same principle of representing an item as a set of disassociated elements. In the fields of Computer Vision and Multimedia, this is referred to as “Bag of Visual Words” (BOVW). Here we will be doing retrieval, but it helps to look at this Wikipedia page, which describes classification:\n",
    "http://en.wikipedia.org/wiki/Bag-of-words_model_in_computer_vision\n",
    "For a small summary of BOW please visit:  https://towardsdatascience.com/bag-of-visual-words-in-a-nutshell-9ceea97ce0fb\n",
    "\n",
    "Each image can be explained as a bag of visual words. Each visual word is a cluster of SIFT visual descriptors. \n",
    "![visualwords](Images/bagoffeatures_visualwordsoverview.png)\n",
    "**source:** Mathworks\n",
    "http://nl.mathworks.com/help/vision/ug/image-classification-with-bag-of-visual-words.html\n",
    "\n",
    "Under the BOVW approach, we create visual words by clustering the SIFT descriptors that we have extracted from the images in our data set using k-means clustering, a commonly used clustering algorithm. We recommend you have a look at the Wikipedia page: http://en.wikipedia.org/wiki/K-means_clustering\n",
    "\n",
    "Applying k-means clustering to the SIFT descriptors results in a set of sets of descriptors (i.e., a set of clusters of descriptors that are similar to each other). The center of each of the clusters is used to represent the cluster. It is treated as a codeword in a codebook. Note that for this reason the centers (and therefore the codewords) have exactly the same form as a regular feature. \n",
    "\n",
    "When a new query image is presented to the CBIR system, we need to create a representation made of visual words, in order to compare that image to images in the background collection. We extract SIFT descriptors from that image. For each descriptor, we check the distance to every codeword in the codebook and return the codeword closest to it (and the distance). We count how often each codeword comes out as the closest match to a feature and use these counts to build our BOVW vector.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1404e06b",
   "metadata": {},
   "source": [
    "### 2.2 Visual word vocabulary and vector representations of the image set\n",
    "We train a visual word vocabulary using a set of images, and then use that vocabulary to create a vector representation of each of those images. \n",
    "\n",
    "Creating vector representations of multimedia content such as images is sometimes called indexing, and the result is an index, which usually implies a special data structure. However, in this lab, we refer to the set of vectors as the database (because we are not using a special data structure.) The set of images will be referred to as the collection or the background collection, but also the database images (because these are the images that are represented by the vectors in the database).\n",
    "\n",
    "We will perform CBIR by matching the vector representation of a query image to the vector representations of each the images in the collection, and returning the top-N closest images (i.e., most similar images).\n",
    "\n",
    "The first step is already done. In other words, we have already trained the SIFT vocabulary and created the image index, which allows you to retrieve images from the image collection (referred to here as the “database”). \n",
    "\n",
    "We are going to use **two different versions of the database. Please pay attention and switch the versions carefully** so that you are using the right version for the right section below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9fbbb9",
   "metadata": {},
   "source": [
    "### 2.3 Information about the databases and images\n",
    "\n",
    "Please check if there is a folder `/Images/joint` with images and databases in the `/Databases` folder. Take a look at the images to understand what the databases contain. The suffixes used are the same as those from the given image collection for this lab (see the Prerquisites). If the images and databases are there, you can skip to the next section to process the images you were given at the start of the lab.\n",
    "\n",
    "Take a look at the images to understand what the databases contain. The direction database contains only the images indicated by a d and the scale database contains only the images indicated by an s. The joint database contains all of the images. The suffixes used are the ones used in the given image collection (see the Prerquisites).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cf48b0",
   "metadata": {},
   "source": [
    "### 2.4 Image pre-processing\n",
    "1. Now, you should create a new folder in the `/studentimages_resized` directory.\n",
    "2. Then, resize those images with the resizing tool here below.\n",
    "\n",
    "The tool is simple function that only does resizing and keeps the meta data of the image. The function has the following inputs:\n",
    "- path_to_images -> this is the absolute path to the folder where the image are that needed to be resized.\n",
    "- withd -> this the new size of the image in pixels.\n",
    "- path_to_output -> this is where the new images will be saved (default = path_to_images).\n",
    "\n",
    "Have a look at the python file in this folder and resize all images to 500 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c4101a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'ls' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "from resize import resize\n",
    "# Start answer here\n",
    "    \n",
    "# End snswer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cfc436",
   "metadata": {},
   "source": [
    "### 2.5 Local vs. Global Features for CBIR\n",
    "First, we will be using the database from the Databases/direction folder, which was created using the direction images. This collection contains images taken by other students in the past years of Multimedia Analysis standing in front of EWI facing different directions.\n",
    "\n",
    "While in the MMA/Code folder, investigate the “help” of ./query.py. Make sure you understand the arguments of this tool. This tool need some extra packages. Go to anaconda terminal and do the following pip installs:\n",
    "- pip install progressbar\n",
    "- pip install progressbar2\n",
    "- pip install exifread\n",
    "- pip install geopy\n",
    "\n",
    "Choose one of the direction (eg. d_W.jpg) photos from the given image collection (see the Prerquisites) as a query. And answer the following questions.\n",
    "\n",
    " 1. What do you expect to be the result when you query the direction database using this query? Why? (Remember that we are using visual word representations of both the query and all of the images in the database, which are local features.)\n",
    "\n",
    "<font color='red'>\n",
    "    ANSWER\n",
    "</font>\n",
    "\n",
    "Run a query passing the necessary arguments and your image.\n",
    "\n",
    " 2. Were your expectations upheld? Are you seeing evidence that visual content-based matching is able to capture similarity between different photos of the same scene? You can save the matplotlib images and add your best results to the report. Describe this evidence. \n",
    "\n",
    "<font color='red'>\n",
    "    ANSWER\n",
    "</font>\n",
    "\n",
    "Now you will try the same query using a database containing color histogram representations of the complete image collection (/joint). \n",
    "\n",
    "In order to experiment with color histogram run:\n",
    "\n",
    "    %run Code/dbt.py -d colorHistDB colorhist ../Images/joint/\n",
    "\n",
    "This code will create a database named colorHistDB.db in the /MMA/Code/db folder. \n",
    "\n",
    " 3. What do you expect to be the result when you query the database using this query? (Remember that we are now using color histogram features, which are global features).\n",
    "\n",
    "Run the query with the colorhist option.\n",
    "\n",
    " 4. Are you seeing evidence that visual content-based matching behaves differently when the images are represented with color histogram features? Again, you can save the matplotlib images and add your best results to the report. Describe what you have found \n",
    "\n",
    "Now download the image son_ewi_4ever.jpg\n",
    "https://drive.google.com/open?id=0BzUoY1B9h0PMQmdDaVVmS21qdVk\n",
    "Use this image as a query, query the joint collection. \n",
    "\n",
    " 5. Do you see evidence of SIFT visual words being invariant to scale and rotation? You can save the matplotlib images and add your evidence to the report. Describe this evidence (or lack thereof) briefly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04274b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Multi Media Analysis Query Tool\n",
      "================================\n",
      "\n",
      "Query the database with [ C:/Users/ardyz/Documents/TU Delft projects/mma-lab/Labs/Lab2_ImageQueries/studentimages/d_W.jpg ] for [ sift ] features...\n",
      "Loading SIFT vocabulary ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[                                                                        ]   0%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating SIFT features for [ 1 ] images ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[========================================================================] 100%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query database with a SIFT histogram...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "OperationalError",
     "evalue": "no such table: sift_imwords",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m~\\Documents\\TU Delft projects\\mma-lab\\Labs\\Lab2_ImageQueries\\Code\\query.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Query database with a SIFT histogram...'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[1;31m# Use the histogram to search the database\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m         \u001b[0msift_candidates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msearch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquery_iw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'sift'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfeature_active\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'harris'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\TU Delft projects\\mma-lab\\Labs\\Lab2_ImageQueries\\Code\\image_search.py\u001b[0m in \u001b[0;36mquery_iw\u001b[1;34m(self, type, h)\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mquery_iw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m         \u001b[1;34m\"\"\" Find a list of matching images for image histogram h\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m         \u001b[0mcandidates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcandidates_from_histogram\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcandidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m         \u001b[0mmatchscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\TU Delft projects\\mma-lab\\Labs\\Lab2_ImageQueries\\Code\\image_search.py\u001b[0m in \u001b[0;36mcandidates_from_histogram\u001b[1;34m(self, type, imwords)\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[0mcandidates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m             \u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcandidates_from_word\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m             \u001b[0mcandidates\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\TU Delft projects\\mma-lab\\Labs\\Lab2_ImageQueries\\Code\\image_search.py\u001b[0m in \u001b[0;36mcandidates_from_word\u001b[1;34m(self, type, imword)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         im_ids = self.con.execute(\n\u001b[1;32m---> 21\u001b[1;33m             \"select distinct imid from \" + type + \"_imwords where wordid=%d\" % imword).fetchall()\n\u001b[0m\u001b[0;32m     22\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mim_ids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOperationalError\u001b[0m: no such table: sift_imwords"
     ]
    }
   ],
   "source": [
    "%run Code/query.py \"C:/Users/ardyz/Documents/TU Delft projects/mma-lab/Labs/Lab2_ImageQueries/Databases/direction/MMA\" \"C:/Users/ardyz/Documents/TU Delft projects/mma-lab/Labs/Lab2_ImageQueries/studentimages/d_W.jpg\" \"sift\"\n",
    "# %run Code/dbt.py \"sift\" \"C:/Users/ardyz/Documents/TU Delft projects/mma-lab/Labs/Lab2_ImageQueries/studentimages/\" -p \"C:/Users/ardyz/Documents/TU Delft projects/mma-lab/Labs/Lab2_ImageQueries/Databases/direction/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610d9bdd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
