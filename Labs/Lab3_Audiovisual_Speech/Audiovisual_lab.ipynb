{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lab is based on a lab from the ECE 417 course of the University of Illinois: https://courses.engr.illinois.edu/ece417/fa2017/mp5.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Audiovisual Speech Recognition\n",
    "\n",
    "## 1.1 Our goals today:\n",
    "\n",
    "This lab will introduce you to the basics of automatic speech recognition using audio features only, image features only, and the combination of both features, i.e., audiovisual speech recognition. The speech recognition system is based on Hidden Markov Models. We will implement part of the training and decoding algorithm. You will train and test the automatic speech recognition system using the \"leave-one-out\" scheme. You will learn to understand the importance of audio, visual, and audiovisual features for speech recognition. \n",
    "\n",
    "\n",
    "## 1.2 Data <i class=\"fa fa-file\"></i>\n",
    "The data that you will use corresponds to 10 instances of the digit \"2\" being spoken and 10 instances of the digit \"5\" being spoken.\n",
    "The audio data consists of MFCC features and the visual data consists of lip tracking features. \n",
    "These features can be found in the data/ folder.\n",
    "Since there are not a lot of instances, there is no split of training and test data. In order to evaluate the performance of your speech recognizer you will be making use of cross validation, specifically the \"leave one out\" scheme, instead of a typical test set. \n",
    "\n",
    "You can read data/README.md for more details on the data.\n",
    "\n",
    "1. Inspect the data. What does the data look like? Describe what the rows and columns represent.\n",
    "\n",
    "2. Compare the audio features and video features of the same instance. What is similar? What is different?\n",
    "\n",
    "3. Now take a look at other instances. What is similar? What is different?\n",
    "\n",
    "\n",
    "## 1.3 Hidden Markov Model\n",
    "The method that you will use to develop your bi-modal speech recognizer is the Hidden Markov Model (HMM):\n",
    "\n",
    "\"\n",
    "> The HMM is based on augmenting the Markov chain. A Markov chain is a model that  tells  us  something  about  the  probabilities  of  sequences  of  random  variables, states, each of which can take on values from some set. These sets can be words, or tags, or symbols representing anything, like the weather.  A Markov chain makes a very strong assumption that if we want to predict the future in the sequence, all that matters is the current state. The states before the current state have no impact on the future except via the current state. It’s as if to predict tomorrow’s weather you could examine today’s weather but you weren’t allowed to look at yesterday’s weather.\n",
    "\n",
    "> A Markov chain is useful when we need to compute a probability for a sequence of observable events. In many cases, however, the events we are interested in are hidden: we don’t observe them directly. For example we don’t normally observe speech units in an audio signal. Rather must infer the speech units from the sequence of audio. We call the speech units hidden because they are not observed. A hidden Markov model(HMM) allows us to talk about both observed events (like an audio sequence) and hidden events (like speech units) that we think of as causal factors in our probabilistic model.\n",
    "\n",
    "> An HMM $W = (A,B)$ is specified by the following components:\n",
    "* $Q=q_1q_2...q_N$ a set of $N$ states\n",
    "* $A=A_{11}...a_{ij}...a_{NN}$ a **transition probability matrix** $A$, each $a_{ij}$ representing the probability of moving from state $i$ to state $j$, s.t. $\\sum^N_{j=1}a_{ij}=1$  $\\forall i$\n",
    "* $O=o_1o_2...o_T$ a sequence of $T$ **observations**, each one drawn from a vocabulary $V=v_1,v_2,...,v_V$\n",
    "* $B=b_i(o_t)$ a sequence of **observation likelihoods**, also called **emission probabilities**, each expressing the probability of an obervation $o_t$ being generated from state $i$\n",
    "* $\\pi=\\pi_1,\\pi_2,...,\\pi_N$ an **initial probability distribution** over states. $\\pi_i$ being the probability that the Markov chain will start in state $i$. Some states $j$ may have $\\pi_j=0$, meaning that they cannot be initial states. Also, $\\sum^n_{i=1}pi_i=1$\n",
    "\n",
    "\"\n",
    "\n",
    "\\- Speech and Language Processing.  Daniel Jurafsky & James H. Martin. Chapter A.\n",
    "\n",
    "This lab focuses on two of the fundamental HMM problems:\n",
    "\n",
    "*  Given an observation sequence $O$ and the set of states in the HMM, learn the HMM probabilities $A$ and $B$, which will allow us to compute the class conditional probability $P(O|W)$, i.e., the acoustic model. In order to train the HMM acoustic models we will use a version of the Expectation-Maximization algorithm called the Forward-Backward algorithm.\n",
    "*  Given an observation sequence $O$ consisting of a sequence of features (audio only, visual only, audio-visual), determine the best-matching digit $P(W|O)$. For testing, we use the Forward algorithm, which is very close to the Viterbi algorithm discussed in class, except that it does not take the max over the history but rather sums over the history.\n",
    "\n",
    "You are provided with code that implements the Expectation-Maximization algorithm.\n",
    "The code is only partially complete however. It is up to you to fill in the gaps.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Training the HMM acoustic model, P(O|W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training the HMM acoustic models we will use the Expectation-Maximization (EM) algorithm (see Figure 6.16 of the book). \n",
    "\n",
    "The EM algorithm consists of the following steps:\n",
    "\n",
    "1. Initialize $A$ and $B$\n",
    "2. Expectation step:\n",
    "  1. Forward-Backward algorithm:\n",
    "      1. Calculate the forward probability $\\alpha$ (Forward step)\n",
    "      2. Calculate the backward probability $\\beta$ (Backward step)\n",
    "  2. Calculate the state occupancy count $\\gamma$ (number of times a state is occupied)\n",
    "  3. Calculate the state transition count $\\xi$ (number of transitions from a state)\n",
    "3. Maximization step\n",
    "  1. Calculate $A$ using $\\xi$\n",
    "  2. Calculate $B$ using $\\gamma$\n",
    "4. Repeat steps 2 and 3 until convergence\n",
    "5. Return $A$ and $B$\n",
    "  \n",
    "The implementation of the EM algorithm below deviates a little bit of the algorithm that is described in the book, but the overal picture is the same.\n",
    "\n",
    "ghmm_train is the function that you will call to train an HMM.\n",
    "\n",
    "Once the acoustic models are trained, these will be used to calculate the class conditional probabilities $P(O|W)$ which we need to determine the most likely digit during testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ghmm_train(Yseq,N,Ainit=None):\n",
    "# Input:\n",
    "#  N - number of states\n",
    "#  Ainit  - This is the initial A matrix that you can input to force the model\n",
    "#           to train a left to right model.\n",
    "#  Yseq - This is the data. \n",
    "\n",
    "# Output:\n",
    "#  P0 - Initial state probability matrix\n",
    "#  A - Transition probability matrix\n",
    "#  mu - Mean matrix\n",
    "#  sigma - Covariance matrix \n",
    "\n",
    "    M = np.shape(Yseq[0])[1]\n",
    "    P0 = np.ones([N,1])/N;\n",
    "    \n",
    "    if Ainit is None:\n",
    "        A = np.random.rand(N,N) + 2 * np.eye(N)\n",
    "        for l in range(N):\n",
    "            A[l,:] = A[l,:]/np.sum(A[l,:]);\n",
    "    else:\n",
    "        A = Ainit\n",
    "        \n",
    "    Nseq = len(Yseq)\n",
    "    datatmp = Yseq[0]\n",
    "    \n",
    "    for seq in range(1,Nseq):\n",
    "        datatmp = np.append(datatmp,Yseq[seq],axis=0)\n",
    "        \n",
    "    M = np.shape(datatmp)[1]\n",
    "    T = np.shape(datatmp)[0]\n",
    "    T1 = int(np.floor(T/N))\n",
    "    \n",
    "    mu = np.zeros([N,M])\n",
    "    sigma = np.zeros([N,M,M])\n",
    "\n",
    "    for i in range(N):\n",
    "        mu[i,:] = np.mean(datatmp[i*T1:(i+1)*T1-1,:],axis=0)\n",
    "        sigma[i,:,:] = np.cov(np.transpose(datatmp))\n",
    "        \n",
    "    for itr in range(25):\n",
    "        params = ghmm_em_obs(Yseq,N,A,P0,mu,sigma)\n",
    "        mu = params[0]\n",
    "        sigma = params[1]\n",
    "        A = ghmm_em_trans(Yseq,N,A,P0,mu,sigma)\n",
    "    return(P0,A,mu,sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <i class=\"fa fa-file\"></i>\n",
    "1. How are the transition probabilities of the HMM (A) initialized?\n",
    "2. How are the means and covariances of the HMM (mu and sigma, used to calculated B) initialized?\n",
    "3. What is the last for loop doing? Look at the other functions to understand the training process. What do you think would happen if you repeat this for loop only one time instead of 25 times? What would happen if you repeat it one million times?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ghmm_em_obs calculates the observation mean and covariances. These are used to calculate the emmission probabilities.\n",
    "\n",
    "Since our observations are multi-dimensional vectors, we need multivariate Gaussians in order to be able to assign a probability to a multi-dimensional vector. A multivariate Gaussian is defined by a mean vector $m$ of dimensionality $D$ and a covariance matrix $S$(igma).\n",
    "During training we need to estimate the mean vector $m$ and the covariance matrix $S$. In practice however, we simply keep a mean and variance for each dimension. \n",
    "\n",
    "You don't need to do anything yourself with this function other than running it once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ghmm_em_obs(Yseq,N,A,P0,mu,sigma):\n",
    "    N_seq = len(Yseq)\n",
    "    eps = 1e-5\n",
    "    M = np.shape(Yseq[0])[1]\n",
    "    museq_unnorm = np.zeros([N,M])\n",
    "    museq_norm = np.zeros([N,M])\n",
    "    sigmaseq_unnorm = np.zeros([N,M,M])\n",
    "    sigmaseq_norm = np.zeros([N,1])\n",
    "    Pxyseq = list([] for _ in range(N_seq))\n",
    "    Pxxyseq = list([] for _ in range(N_seq))\n",
    "    llseq = list([] for _ in range(N_seq))\n",
    "    \n",
    "    for seq in range(N_seq):\n",
    "        Y = Yseq[seq]\n",
    "        M = np.shape(Y)[1]\n",
    "        T = np.shape(Y)[0]\n",
    "\n",
    "        fwd = ghmm_fwd(Y,A,P0,mu,sigma)\n",
    "        alpha = fwd[0]\n",
    "        scale = fwd[1]\n",
    "        beta = ghmm_bwd(Y,A,P0,mu,sigma,scale)\n",
    "        probs = ghmm_prob(alpha,beta,scale,A,mu,sigma,Y)\n",
    "        Pxy = probs[0]\n",
    "        Pxxy = probs[1]\n",
    "        Pxyseq[seq] = Pxy\n",
    "        Pxxyseq[seq] = Pxxy\n",
    "        llseq[seq] = np.exp(np.sum(scale))\n",
    "        \n",
    "        for i in range(N):\n",
    "            Pxy0 = Pxy[:,i].reshape(-1,1)\n",
    "            Pxy0 = np.sum(np.multiply(np.repeat(Pxy0,M,axis=1),Y),axis=0)+eps\n",
    "            museq_unnorm[i,:] = np.add(museq_unnorm[i,:],Pxy0)\n",
    "            museq_norm[i,:] = np.add(museq_norm[i,:],np.sum(Pxy[:,i])+eps)\n",
    "            \n",
    "    mu = np.divide(museq_unnorm,museq_norm)\n",
    "    sigmaseq_unnorm = np.zeros([N,M,M])\n",
    "    sigmaseq_norm = np.zeros([N,1])\n",
    "    for seq in range(N_seq):\n",
    "        Y = Yseq[seq]\n",
    "        M = np.shape(Y)[1]\n",
    "        T = np.shape(Y)[0]\n",
    "\n",
    "        fwd = ghmm_fwd(Y,A,P0,mu,sigma)\n",
    "        alpha = fwd[0]\n",
    "        scale = fwd[1]\n",
    "        beta = ghmm_bwd(Y,A,P0,mu,sigma,scale)\n",
    "        probs = ghmm_prob(alpha,beta,scale,A,mu,sigma,Y)\n",
    "        Pxy = probs[0]\n",
    "        Pxxy = probs[1]\n",
    "        Pxyseq[seq] = Pxy\n",
    "        Pxxyseq[seq] = Pxxy\n",
    "        llseq[seq] = np.exp(np.sum(scale));\n",
    "    \n",
    "        for i in range(N):\n",
    "            Pxy0 = Pxy[:,i]\n",
    "            sigmatmp = np.zeros([M,M])\n",
    "            for t in range(T):\n",
    "                sigmatmp = np.add(sigmatmp, np.multiply(np.transpose(Y[t,:] - mu[i,:]).reshape(-1,1), np.multiply(Y[t,:] - mu[i,:],Pxy0[t])))\n",
    "                sigmaseq_norm[i] = sigmaseq_norm[i] + Pxy0[t]\n",
    "            sigmaseq_unnorm[i,:,:] = sigmaseq_unnorm[i,:,:] + np.reshape(sigmatmp,[1,M,M],order='F')\n",
    "            \n",
    "    for i in range(N):\n",
    "        sigma[i,:,:] = np.divide(sigmaseq_unnorm[i,:,:],sigmaseq_norm[i])\n",
    "    return(mu,sigma)                                              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Expectation step <i class=\"fa fa-file\"></i>\n",
    "This is the expectation step of the EM algorithm. Here ghmm_prob calculates two probabilities:\n",
    "\n",
    "First is the probability of being in state $S_i$ at time $t$, given the observation sequence $O$ and the model $W$:\n",
    "\n",
    "$\\gamma_t(i)= P(q_t=S_i|O,W) = \\frac{\\alpha_t(i)\\beta_t(i)}{P(O|W)} = \\frac{\\alpha_t(i)\\beta_t(i)}{\\sum_{i=1}^N\\alpha_t(i)\\beta_t(i)}$\n",
    "\n",
    "You will need to insert a line of code into this function that computes $\\gamma_t(i)$ ( i.e. Pxy ) when you are given $\\alpha_t(i)\\beta_t(i)$. Put that code in your Google Document. \n",
    "\n",
    "This probability is used by ghmm_em_obs to calculate the observation mean and covariances, which are used to calculated the emission probabilities.\n",
    "\n",
    "Second is the probability of being in state $S_i$ at time $t$ and $S_j$ at time $t+1$, given the observation sequence $O$ and the model $W$:\n",
    "\n",
    "$\\xi_t(i,j)= P(q_t=S_i,q_{t+1}=S_j|O,W) = \\frac{\\alpha_t(i)a_{ij}b_j(O_{t+1})\\beta_t(i)}{P(O|W)} = \\frac{\\alpha_t(i)a_{ij}b_j(O_{t+1})\\beta_t(i)}{\\sum_{i=1}^N\\sum_{j=1}^N\\alpha_t(i)a_{ij}b_j(O_{t+1})\\beta_t(i)}$\n",
    "\n",
    " You will need to insert another line of code into this function that computes $\\xi_t(i,j)$ ( i.e. Pxxy ) when you are given $\\alpha_t(i)a_{ij}b_j(O_{t+1})\\beta_t(i)$. \n",
    " \n",
    " Put the code in your Google Document. \n",
    "\n",
    "This probability is used by ghmm_em_trans to calculate the transition probabilities between HMM states.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ghmm_prob(alpha,beta,scale,A,mu,sigma,Y):\n",
    "# Input:\n",
    "#   alpha - forward probability (explained in 2.1.2)\n",
    "#   beta - backward probability (explained in 2.1.3)\n",
    "#   Y - observation vector sequence \n",
    "#   A - state transition probabilities\n",
    "#   P0 - initial state probabilities \n",
    "#   mu - observation mean\n",
    "#   sigma - observation variance\n",
    "\n",
    "# Output:\n",
    "#  Pxy = P(x_t|Y)\n",
    "#  Pxxy = P(x_t,x_{t+1}|Y)\n",
    "\n",
    "    scale = np.exp(scale)\n",
    "    N = np.shape(A)[0]\n",
    "    i = np.ones([1,N])\n",
    "    T = np.shape(Y)[0]\n",
    "    M = np.shape(Y)[1]\n",
    "    \n",
    "    Pys = np.zeros([N,T])\n",
    "\n",
    "    for j in range(N):\n",
    "        sigmai = np.reshape(sigma[j,:,:],(M,M),order='F')\n",
    "        for t in range(T):\n",
    "            Pys[j,t] = np.sqrt(1/(((2*np.pi)**M)*np.linalg.det(sigmai)))*np.exp(np.dot(-0.5*(Y[t,:]-mu[j,:]),np.dot(np.linalg.inv(sigmai),(Y[t,:]-mu[j,:]).reshape(-1,1))))\n",
    "    Pxy = np.zeros([T,N])\n",
    "    for t in range(T):\n",
    "        Pxy0 = np.multiply(alpha[:,t],np.exp(beta[:,t])*scale[t])\n",
    "        Pxy[t,:] = ???\n",
    "        \n",
    "    Pxxy = np.zeros([T,N,N])\n",
    "    for t in range(T-1):\n",
    "        Pxxy0 = np.multiply(alpha[:,t].reshape(-1,1) * np.transpose(np.multiply(np.exp(beta[:,t+1]),Pys[:,t+1])),A)\n",
    "        Pxxy[t,:,:]= ???\n",
    "    return(Pxy,Pxxy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 Forward step <i class=\"fa fa-file\"></i>\n",
    "\n",
    "This is the forward step of the forward-backward algorithm. It calculates the forward probability:\n",
    "\n",
    "$\\alpha_t(i)=P(O_1O_2...O_t,q_t=S_i|W)$\n",
    "\n",
    "which is the probability of a partial observation sequence $O_1O_2...O_t$ and state $S_i$ at time $t$, given the model $W$.\n",
    "\n",
    "You will need to insert a line of code into ghmm_fwd that computes the forward probability (i.e. alpha\\_t) from the transition probabilities A, emission probabilities Pys and the previous forward probability alpha[:,t-1]:\n",
    "\n",
    "$\\alpha_{t}(j) = [\\sum^N_{i=1} \\alpha_{t-1}(i)a_{ij}] b_j (O_{t}), 1<t\\leq T-1, 1\\leq j \\leq N$\n",
    "\n",
    "The initialization $\\alpha_1(i) = \\pi_1b_i(O_1),1\\leq i\\leq N$ has already been done.\n",
    "\n",
    "Put the code in your Google Document.\n",
    "\n",
    "Once you have trained an HMM you can use this function to determine the likelihood of an observation sequence belonging to that HMM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ghmm_fwd(Y,A,P0,mu,sigma):\n",
    "# Input:\n",
    "#   Y - observation vector sequence\n",
    "#   A = state transition probabilities\n",
    "#   P0 - initial state probabilities\n",
    "#   mu - observation mean\n",
    "#   sigma - observation variance\n",
    "\n",
    "# Output:\n",
    "#   alpha - forward probability (explained in 2.1.2)\n",
    "#   scale - log Pr( O_1, O_2, ..., O_t | W), 1 < t <= T\n",
    "    N = np.shape(A)[0]\n",
    "    i = np.ones([1,N])\n",
    "    T = np.shape(Y)[0]\n",
    "    M = np.shape(Y)[1]\n",
    "    alpha = np.zeros([np.shape(P0)[0],T])\n",
    "    Pys = np.zeros([N,T])\n",
    "    \n",
    "    for j in range(N):\n",
    "        sigmai = np.reshape(sigma[j,:,:],(M,M),order='F')\n",
    "        for t in range(T):\n",
    "            Pys[j,t] = np.sqrt(1/(((2*np.pi)**M)*np.linalg.det(sigmai)))*np.exp(np.dot(-0.5*(Y[t,:]-mu[j,:]),np.dot(np.linalg.inv(sigmai),(Y[t,:]-mu[j,:]).reshape(-1,1))))\n",
    "\n",
    "    alpha0 = np.multiply(P0,Pys[:,0].reshape(-1,1))\n",
    "    scale = np.zeros([T,1])\n",
    "    scale[0] = np.sum(alpha0)    \n",
    "    alpha[:,0] = (alpha0 / scale[0]).flatten()\n",
    "    \n",
    "    for t in range(1,T):\n",
    "        # Hint: A 1-d numpy vector is typically (1,n) and cannot be transposed. \n",
    "        #  You can use .reshape(-1,1) to make it (n,1). \n",
    "        #  For matrices you can simply use np.transpose.\n",
    "        alpha_t = ???\n",
    "        scale[t] = np.sum(alpha_t)\n",
    "        alpha[:,t] = alpha_t / scale[t]\n",
    "        \n",
    "    scale = np.log(scale)\n",
    "    return (alpha,scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 Backward step <i class=\"fa fa-file\"></i>\n",
    "\n",
    "This is the backward step of the forward-backward algorithm. It calculates the backward probability:\n",
    "\n",
    "$\\beta_t(i) = P(O_{t+1}O_{t+2}...O_T|q_t=S_i,W)$\n",
    "\n",
    "which is the probability of the partial observation sequence from $t+1$ to $T$, given state $S_i$ at time $t$ and the model $W$\n",
    "\n",
    "You will need to insert a line of code into ghmm_bwd that computes the current backward probability (i.e. beta[:,t]) from the transition probabilities A, emission probabilities Pys, and the previous backward probability beta[:,t+1]:\n",
    "\n",
    "$\\beta_t(i) = \\sum^N_{j=1}a_{ij}b_j(O_{t+1})\\beta_{t+1}(j),t=T-1,T-2,...,1, 1 \\leq i \\leq N$\n",
    "\n",
    "The initialization $\\beta_T(i) = 1,1\\leq i\\leq N$ has already been done.\n",
    "\n",
    "Put the code in your Google Document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ghmm_bwd(Y,A,P0,mu,sigma,scale):\n",
    "# Input:\n",
    "#   T - observation vector sequence\n",
    "#   A = state transition probabilities\n",
    "#   P0 - initial state probabilities\n",
    "#   mu - observation mean\n",
    "#   sigma - observation variance\n",
    "#   scale - log Pr( O_1, O_2, ..., O_t | W), 1 < t <= T (see 'ghmm_fwd')\n",
    "\n",
    "# Output:\n",
    "#   beta - backward probability (explained in 2.1.3)\n",
    "\n",
    "    scale = np.exp(scale)\n",
    "    N = np.shape(A)[0];\n",
    "    T = np.shape(Y)[0]\n",
    "    M = np.shape(Y)[1]\n",
    "    Pys = np.zeros([N,T])\n",
    "    \n",
    "    for j in range(N):\n",
    "        sigmai = np.reshape(sigma[j,:,:],[M,M],order='F')\n",
    "        for t in range(T):\n",
    "            Pys[j,t] = np.sqrt(1/(((2*np.pi)**M)*np.linalg.det(sigmai)))*np.exp(np.dot(-0.5*(Y[t,:]-mu[j,:]),np.dot(np.linalg.inv(sigmai),(Y[t,:]-mu[j,:]).reshape(-1,1))))\n",
    "            \n",
    "    beta = np.zeros([N,T])\n",
    "    beta[:,T-1] = (np.ones([N,1])/scale[T-1]).flatten()\n",
    "    for t in range(T-2,-1,-1):\n",
    "        beta[:,t] = ???\n",
    "        beta[:,t] = beta[:,t] / scale[t]\n",
    "    beta = np.log(beta+1.0e-10)\n",
    "    \n",
    "    return beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.4 Maximization step <i class=\"fa fa-file\"></i>\n",
    "\n",
    "This is the maximization step of the EM algorithm. Here ghmm_em_trans trains the probabilities of the transition matrix.\n",
    "\n",
    "You will need to insert a line of code into this function that computes the new transition probabilities A[k,:] from the corresponding soft-count matrix Naseq[k,:]:\n",
    "\n",
    "$\\hat{a}_{ij} = \\frac{\\textrm{Expected number of transitions from state S_i to state S_j}}{\\textrm{Expected number of transitions from state S_i}} = \\frac{\\sum_{t=1}^{T-1} \\xi_t(i,j)}{\\sum_{t=1}^{T-1}\\sum_{k=1}^N \\xi_t(i,k)}$\n",
    "\n",
    "Put the code in your Google Document.\n",
    "\n",
    "These transition probabilities are trained over several iterations and are needed in order to determine the likelihood of an obervation belonging to an HMM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ghmm_em_trans(Yseq,N,A,P0,mu,sigma):\n",
    "    N_seq = np.max(np.shape(Yseq))\n",
    "    MAX_ITR = 20\n",
    "    eps=0\n",
    "    Naseq = np.zeros([N,N])\n",
    "    \n",
    "    for seq in range(N_seq):\n",
    "        Y = Yseq[seq]\n",
    "        M = np.shape(Y)[1]\n",
    "        T = np.shape(Y)[0]\n",
    "        fwd = ghmm_fwd(Y,A,P0,mu,sigma)\n",
    "        alpha = fwd[0]\n",
    "        scale = fwd[1]\n",
    "        beta = ghmm_bwd(Y,A,P0,mu,sigma,scale)\n",
    "        probs = ghmm_prob(alpha,beta,scale,A,mu,sigma,Y)\n",
    "        Pxy = probs[0]\n",
    "        Pxxy = probs[1]\n",
    "        \n",
    "        Na = np.zeros([N,N])\n",
    "        for i in range(N):\n",
    "            for j in range(N):\n",
    "                Na[i,j] = np.sum(Pxxy[:,i,j])\n",
    "        Na = Na+eps\n",
    "        Naseq = Naseq + Na\n",
    "    Anew = np.zeros([N,N])\n",
    "    for k in range(N):\n",
    "        Anew[k,:] = ???\n",
    "    return Anew"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Test an unknown input, argmax P(W|O) <i class=\"fa fa-file\"></i>\n",
    "Now that you can train an HMM, you can use its parameters with ghmm_fwd to predict the likelihood of a sequence belonging to that HMM. Make sure you read the description of the $scale$ variable.\n",
    "\n",
    "You now have to evaluate how well the HMMs can distinguish between the utterances of the digit \"2\" and of the digit \"5\". Since there is no testing data, you will instead make use of the \"leave one out\" scheme. \n",
    "\n",
    "There are a total of 20 utterances. Train an HMM for the digit \"2\" and another for the digit \"5\" using 19 utterances of the data and test it on the single utterance that you have left out.  Repeat this 20 times until you have tested all 20 utterances. \n",
    "\n",
    "Do this procedure for:\n",
    "* Only the video features\n",
    "* Only the audio features\n",
    "* The audio and video features combined (concatenated).\n",
    "\n",
    "For each of these sets of features, determine the performance (separately) by calculating the accuracy: $\\frac{\\#correct}{20}$. Put the three accuracies in your document. Include the \"leave one out\" code as well.\n",
    "\n",
    "Please note: Depending on your machine and system, the audio features can take a long time to train on. Running the code in the VM that is provided for this course can significantly speed up the training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"data\"\n",
    "N = 5\n",
    "Ainit = np.array([[0.8,0.2,0,0,0],[0,0.8,0.2,0,0],[0,0,0.8,0.2,0],[0,0,0,0.8,0.2],[0,0,0,0,1]])\n",
    "\n",
    "# Enter your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <i class=\"fa fa-file\"></i>\n",
    "\n",
    "1. Which type of data performs better? \n",
    "2. What do you think could be a reason for that?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
