{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audiovisual Speech Recognition\n",
    "In this lab you will develop a bi-modal speech recognizer that makes use of both audio and visual features.\n",
    "\n",
    "## Data\n",
    "The data that you will use corresponds to 10 utterances of the digit \"2\" and 10 utterances of the digit \"5\". \n",
    "The feature extraction has already been done.\n",
    "These features can be found in the data/ folder. \n",
    "It contains both speech features and visual lip tracking features for all utterances. \n",
    "You can read data/README.md for more details about the content of the data.\n",
    "\n",
    "## Hidden Markov Model\n",
    "The method that you will use to develop your bi-modal speech recognizer is the Hidden Markov Model(HMM):\n",
    "\n",
    "> The HMM is based on augmenting the Markov chain. A Markov chain is a model that  tells  us  something  about  the  probabilities  of  sequences  of  random  variables,states, each of which can take on values from some set. These sets can be words, or tags, or symbols representing anything, like the weather.  A Markov chain makes a very strong assumption that if we want to predict the future in the sequence, all that matters is the current state. The states before the current state have no impact on the future except via the current state. It’s as if to predict tomorrow’s weather you could examine today’s weather but you weren’t allowed to look at yesterday’s weather.\n",
    "\n",
    "> A Markov chain is useful when we need to compute a probability for a sequence of observable events. In many cases, however, the events we are interested in are hidden: we don’t observe them directly. For example we don’t normally observe hidden part-of-speech tags in a text. Rather, we see words, and must infer the tags from the word sequence. We call the tags hidden because they are not observed. A hidden Markov model(HMM) allows us to talk about both observed events (like words that we see in the input) and hidden events (like part-of-speech tags) that we think of as causal factors in our probabilistic model.\n",
    "\n",
    "> An HMM is specified by the following components:\n",
    "* $Q=q_1q_2...q_N$ a set of $N$ states\n",
    "* $A=A_{11}...a_{ij}...a_{NN}$ a **transition probability matrix** $A$, each $a_{ij}$ representing the probability of moving from state $i$ to state $j$, s.t. $\\sum^N_{j=1}a_{ij}=1$  $\\forall i$\n",
    "* $O=o_1o_2...o_T$ a sequence of $T$ **observations**, each one drawn from a vocabulary $V=v_1,v_2,...,v_V$\n",
    "* $B=b_i(o_t)$ a sequence of **observation likelihoods**, also called **emission probabilities**, each expressing the probability of an obervation $o_t$ being generated from state $i$\n",
    "* $\\pi=\\pi_1,\\pi_2,...,\\pi_N$ an **initial probability distribution** over states. $\\pi_i$ being the probability that the Markov chain will start in state $i$. Some states $j$ may have $\\pi_j=0$, meaning that they cannot be initial states. Also, $\\sum^n_{i=1}pi_i=1$\n",
    "\n",
    "This lab focuses on two of the fundamental HMM problems:\n",
    "\n",
    "*  Learning: Given an observation sequence $O$ and the set of states in the HMM, learn the HMM parameters A and B. \n",
    "*  Likelihood: Given a HMM $\\lambda = (A,B)$ and on observation sequence $O$, determine the likelihood $P(O|\\lambda)$.\n",
    "\n",
    "You are provided with code that implements the EM algorithm and the Forward-Backward algorithm which solve these problems.\n",
    "The code is only partially complete however. It is up to you to fill in the gaps.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ghmm_learn is the function that you will call to learn a HMM.\n",
    "\n",
    "You will later use its output parameters to calculate the likelihood that a sequence belongs to that HMM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ghmm_learn(Yseq,N,Ainit=None):\n",
    "# Input:\n",
    "#  N - number of states\n",
    "#  Ainit  - This is the initial A matrix that you can input to force the model\n",
    "#           to learn a left to right model.\n",
    "#  Yseq - This is the data. \n",
    "\n",
    "# Output:\n",
    "#  P0 - Initial state probability matrix\n",
    "#  A - Transition probability matrix\n",
    "#  mu - Mean matrix\n",
    "#  sigma - Covariance matrix \n",
    "\n",
    "    M = np.shape(Yseq[0])[1]\n",
    "    P0 = np.ones([N,1])/N;\n",
    "    \n",
    "    if Ainit is None:\n",
    "        A = np.random.rand(N,N) + 2 * np.eye(N)\n",
    "        for l in range(N):\n",
    "            A[l,:] = A[l,:]/np.sum(A[l,:]);\n",
    "    else:\n",
    "        A = Ainit\n",
    "        \n",
    "    Nseq = len(Yseq)\n",
    "    datatmp = Yseq[0]\n",
    "    \n",
    "    for seq in range(1,Nseq):\n",
    "        datatmp = np.append(datatmp,Yseq[seq],axis=0)\n",
    "        \n",
    "    M = np.shape(datatmp)[1]\n",
    "    T = np.shape(datatmp)[0]\n",
    "    T1 = int(np.floor(T/N))\n",
    "    \n",
    "    mu = np.zeros([N,M])\n",
    "    sigma = np.zeros([N,M,M])\n",
    "\n",
    "    for i in range(N):\n",
    "        mu[i,:] = np.mean(datatmp[i*T1:(i+1)*T1-1,:],axis=0)\n",
    "        sigma[i,:,:] = np.cov(np.transpose(datatmp))\n",
    "        \n",
    "    for itr in range(25):\n",
    "        params = ghmm_em_obs(Yseq,N,A,P0,mu,sigma)\n",
    "        mu = params[0]\n",
    "        A = ghmm_em_trans(Yseq,N,A,P0,mu,sigma)\n",
    "    return(P0,A,mu,sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ghmm_em_obs calculates the observation mean and covariances.\n",
    "\n",
    "You don't need to do anything yourself with this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ghmm_em_obs(Yseq,N,A,P0,mu,sigma):\n",
    "    N_seq = np.max(np.shape(Yseq))\n",
    "    eps = 1e-5\n",
    "    M = np.shape(Yseq[0])[1]\n",
    "    museq_unnorm = np.zeros([N,M])\n",
    "    museq_norm = np.zeros([N,M])\n",
    "    sigmaseq_unnorm = np.zeros([N,M,M])\n",
    "    sigmaseq_norm = np.zeros([N,1])\n",
    "    Pxyseq = list([] for _ in range(N_seq))\n",
    "    Pxxyseq = list([] for _ in range(N_seq))\n",
    "    llseq = list([] for _ in range(N_seq))\n",
    "    \n",
    "    for seq in range(N_seq):\n",
    "        Y = Yseq[seq]\n",
    "        M = np.shape(Y)[1]\n",
    "        T = np.shape(Y)[0]\n",
    "\n",
    "        fwd = ghmm_fwd(Y,A,P0,mu,sigma)\n",
    "        alpha = fwd[0]\n",
    "        scale = fwd[1]\n",
    "        beta = ghmm_bwd(Y,A,P0,mu,sigma,scale)\n",
    "        probs = ghmm_prob(alpha,beta,scale,A,mu,sigma,Y)\n",
    "        Pxy = probs[0]\n",
    "        Pxxy = probs[1]\n",
    "        Pxyseq[seq] = Pxy\n",
    "        Pxxyseq[seq] = Pxxy\n",
    "        llseq[seq] = np.exp(np.sum(scale))\n",
    "        \n",
    "        for i in range(N):\n",
    "            Pxy0 = Pxy[:,i].reshape(-1,1)\n",
    "            Pxy0 = np.sum(np.multiply(np.repeat(Pxy0,M,axis=1),Y),axis=0)+eps\n",
    "            museq_unnorm[i,:] = np.add(museq_unnorm[i,:],Pxy0)\n",
    "            museq_norm[i,:] = np.add(museq_norm[i,:],np.sum(Pxy[:,i])+eps)\n",
    "            \n",
    "    mu = np.divide(museq_unnorm,museq_norm)\n",
    "    sigmaseq_unnorm = np.zeros([N,M,M])\n",
    "    sigmaseq_norm = np.zeros([N,1])\n",
    "    for seq in range(N_seq):\n",
    "        Y = Yseq[seq]\n",
    "        M = np.shape(Y)[1]\n",
    "        T = np.shape(Y)[0]\n",
    "\n",
    "        fwd = ghmm_fwd(Y,A,P0,mu,sigma)\n",
    "        alpha = fwd[0]\n",
    "        scale = fwd[1]\n",
    "        beta = ghmm_bwd(Y,A,P0,mu,sigma,scale)\n",
    "        probs = ghmm_prob(alpha,beta,scale,A,mu,sigma,Y)\n",
    "        Pxy = probs[0]\n",
    "        Pxxy = probs[1]\n",
    "        Pxyseq[seq] = Pxy\n",
    "        Pxxyseq[seq] = Pxxy\n",
    "        llseq[seq] = np.exp(np.sum(scale));\n",
    "    \n",
    "        for i in range(N):\n",
    "            Pxy0 = Pxy[:,i]\n",
    "            sigmatmp = np.zeros([M,M])\n",
    "            for t in range(T):\n",
    "                sigmatmp = np.add(sigmatmp, np.multiply(np.transpose(Y[t,:] - mu[i,:]).reshape(-1,1), np.multiply(Y[t,:] - mu[i,:],Pxy0[t])))\n",
    "                sigmaseq_norm[i] = sigmaseq_norm[i] + Pxy0[t]\n",
    "            sigmaseq_unnorm[i,:,:] = sigmaseq_unnorm[i,:,:] + np.reshape(sigmatmp,[1,M,M],order='F')\n",
    "            \n",
    "    for i in range(N):\n",
    "        sigma[i,:,:] = np.divide(sigmaseq_unnorm[i,:,:],sigmaseq_norm[i])\n",
    "    return(mu,sigma)                                              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ghmm_prob calculates two types of probabilities:\n",
    "\n",
    "First is the probability of being in state $S_i$ at time $t$, given the observation sequence $O$ and the model $\\lambda$:\n",
    "\n",
    "$\\gamma_t(i)= P(q_t=S_i|0,\\lambda) = \\frac{\\alpha_t(i)\\beta_t(i)}{P(O|\\lambda)} = \\frac{\\alpha_t(i)\\beta_t(i)}{\\sum_{i=1}^N\\alpha_t(i)\\beta_t(i)}$\n",
    "\n",
    "You will need to insert a line of code into this function that computes p(x|y) from p(x,y) where x is the state index and y is the observation.\n",
    "\n",
    "Second is the probability of being in state $S_i$ ad time $t$ and $S_j$ at time $t+1$, given the observation sequence $O$ and the model $\\lambda$:\n",
    "\n",
    "$\\xi_t(i,j)= P(q_t=S_i,q_{t+1}=S_j|0,\\lambda) = \\frac{\\alpha_t(i)a_{ij}b_j(O_{t+1})\\beta_t(i)}{P(O|\\lambda)} = \\frac{\\alpha_t(i)a_{ij}b_j(O_{t+1})\\beta_t(i)}{\\sum_{i=1}^N\\sum_{j=1}^N\\alpha_t(i)a_{ij}b_j(O_{t+1})\\beta_t(i)}$\n",
    "\n",
    "You will need to insert another line of code into this function that computes p(x(t),x(t+1)|Y) from p(x(t),x(t+1),Y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ghmm_prob(alpha,beta,scale,A,mu,sigma,Y):\n",
    "# Input:\n",
    "#   Y - observation vector sequence \n",
    "#   A - state transition probabilities\n",
    "#   P0 - initial state probabilities \n",
    "#   mu - observation mean\n",
    "#   sigma - observation variance: diagonal\n",
    "\n",
    "# Output:\n",
    "#  Pxy = P(x_t|Y)\n",
    "#  Pxxy = P(x_t,x_{t+1}|Y)\n",
    "\n",
    "    scale = np.exp(scale)\n",
    "    N = np.shape(A)[0]\n",
    "    i = np.ones([1,N])\n",
    "    T = np.shape(Y)[0]\n",
    "    M = np.shape(Y)[1]\n",
    "    \n",
    "    Pys = np.zeros([N,T])\n",
    "\n",
    "    for j in range(N):\n",
    "        sigmai = np.reshape(sigma[j,:,:],(M,M),order='F')\n",
    "        for t in range(T):\n",
    "            Pys[j,t] = np.sqrt(1/(((2*np.pi)**M)*np.linalg.det(sigmai)))*np.exp(np.dot(-0.5*(Y[t,:]-mu[j,:]),np.dot(np.linalg.inv(sigmai),(Y[t,:]-mu[j,:]).reshape(-1,1))))\n",
    "    Pxy = np.zeros([T,N])\n",
    "    for t in range(T):\n",
    "        Pxy0 = np.multiply(alpha[:,t],np.exp(beta[:,t])*scale[t])\n",
    "        Pxy[t,:] = ???\n",
    "        \n",
    "    Pxxy = np.zeros([T,N,N])\n",
    "    for t in range(T-1):\n",
    "        Pxxy0 = np.multiply(alpha[:,t].reshape(-1,1) * np.transpose(np.multiply(np.exp(beta[:,t+1]),Pys[:,t+1])),A)\n",
    "        Pxxy[t,:,:]= ???\n",
    "    return(Pxy,Pxxy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ghmm_em_trans learns the parameters of the transition matrix, using the observation mean and covariance that have been calculated by ghmm_em_obs.\n",
    "\n",
    "You will need to insert a line of code into this function that computes the new transition probabilities A[k,m] from the corresponding soft-count matrix Na[k,m]:\n",
    "\n",
    "$\\hat{a}_{ij} = \\frac{\\textrm{Expected number of transitions from state S_i to state S_j}}{\\textrm{expected number of transitions from state S_i}} = \\frac{\\sum_{t=1}^{T-1} \\xi_t(i,j)}{\\sum_{t=1}^{T-1}\\sum_{k=1}^N \\xi_t(i,k)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ghmm_em_trans(Yseq,N,A,P0,mu,sigma):\n",
    "    N_seq = np.max(np.shape(Yseq))\n",
    "    eps = 0\n",
    "    MAX_ITR = 20\n",
    "    Naseq = np.zeros([N,N])\n",
    "    \n",
    "    for seq in range(N_seq):\n",
    "        Y = Yseq[seq]\n",
    "        M = np.shape(Y)[1]\n",
    "        T = np.shape(Y)[0]\n",
    "        fwd = ghmm_fwd(Y,A,P0,mu,sigma)\n",
    "        alpha = fwd[0]\n",
    "        scale = fwd[1]\n",
    "        beta = ghmm_bwd(Y,A,P0,mu,sigma,scale)\n",
    "        probs = ghmm_prob(alpha,beta,scale,A,mu,sigma,Y)\n",
    "        Pxy = probs[0]\n",
    "        Pxxy = probs[1]\n",
    "        \n",
    "        Na = np.zeros([N,N])\n",
    "        for i in range(N):\n",
    "            for j in range(N):\n",
    "                Na[i,j] = np.sum(Pxxy[:,i,j])\n",
    "        \n",
    "        Na = Na+eps\n",
    "        Naseq = Naseq + Na\n",
    "    Anew = np.zeros([N,N])\n",
    "    for k in range(N):\n",
    "        Anew[k,:] = ???\n",
    "    return Anew"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The forward step of the forward-backward algorithm. This calculates:\n",
    "\n",
    "$\\alpha_t(i)=P(O_1O_2...O_t,q_t=S_i|\\lambda)$\n",
    "\n",
    "which is the probability of a partial observation sequence $O_1O_2...O_t$ and state $S_i$ at time $t$, given the model $\\lambda$.\n",
    "\n",
    "You will need to insert a line of code into ghmm_fwd that computes alpha0 from transition probabilities A, Gaussian probabilities Pys and alpha[t-1]:\n",
    "\n",
    "$\\alpha_{t+1} = [\\sum^N_{i=1} \\alpha_t(i)a_{ij}] b_j (O_{t+1}), 1\\leq T-1, 1\\leq j \\leq N$\n",
    "\n",
    "The initialization $\\alpha_1(i) = \\pi_1b_i(O_1),1\\leq i\\leq N$ has already been done.\n",
    "\n",
    "Once you have learned a HMM you can use this function to determine the likelihood of an observation sequence belonging to that HMM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ghmm_fwd(Y,A,P0,mu,sigma):\n",
    "# Input:\n",
    "#   T - observation vector sequence\n",
    "#   A = state transition probabilities\n",
    "#   P0 - initial state probabilities\n",
    "#   mu - observation mean\n",
    "#   sigma - observation variance\n",
    "\n",
    "# Output:\n",
    "#   alpha - E[ s_t | Y_t ] = Pr( s_t | observation sequence up to time 't')\n",
    "#   scale - log Pr( s_0, ..., s_t | observation sequence up to time 't')\n",
    "    N = np.shape(A)[0]\n",
    "    i = np.ones([1,N])\n",
    "    T = np.shape(Y)[0]\n",
    "    M = np.shape(Y)[1]\n",
    "    alpha = np.zeros([np.shape(P0)[0],T])\n",
    "    Pys = np.zeros([N,T])\n",
    "    \n",
    "    for j in range(N):\n",
    "        sigmai = np.reshape(sigma[j,:,:],(M,M),order='F')\n",
    "        for t in range(T):\n",
    "            Pys[j,t] = np.sqrt(1/(((2*np.pi)**M)*np.linalg.det(sigmai)))*np.exp(np.dot(-0.5*(Y[t,:]-mu[j,:]),np.dot(np.linalg.inv(sigmai),(Y[t,:]-mu[j,:]).reshape(-1,1))))\n",
    "\n",
    "    alpha0 = np.multiply(P0,Pys[:,0].reshape(-1,1))\n",
    "    scale = np.zeros([T,1])\n",
    "    scale[0] = np.sum(alpha0)    \n",
    "    alpha[:,0] = (alpha0 / scale[0]).flatten()\n",
    "    \n",
    "    for t in range(1,T):\n",
    "        # Hint: Use np.reshape(-1,1) to make a 2-d vector from a 1-d vector. \n",
    "        #   This changes how numpy handles certain operations such as '*'.\n",
    "        alpha0 = ???\n",
    "        scale[t] = np.sum(alpha0)\n",
    "        alpha[:,t] = alpha0 / scale[t]\n",
    "        \n",
    "    scale = np.log(scale)\n",
    "    return (alpha,scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The backward step of the forward-backward algorithm. This calculates:\n",
    "\n",
    "$\\beta_t(i) = P(O_{t+1}O_{t+2}...O_T|q_t=S_i,\\lambda)$\n",
    "\n",
    "which is the probability of the partial observation sequence from $t+1$ to $T$, given state $S_i$ at time $t$ and the model $\\lambda$\n",
    "\n",
    "You will need to insert a line of code into ghmm_bwd that computes beta from transition probabilities A, Gaussian probabilities Pys, and beta(t+1):\n",
    "\n",
    "$\\beta_t(i) = \\sum^N_{j=1}a_{ij}b_j(O_{t+1})\\beta_{t+1}(j),t=T-1,T-2,...,1, 1 \\leq i \\leq N$\n",
    "\n",
    "The initialization $\\beta_T(i) = 1,1\\leq i\\leq N$ has already been done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ghmm_bwd(Y,A,P0,mu,sigma,scale):\n",
    "# Input:\n",
    "#   T - observation vector sequence\n",
    "#   A = state transition probabilities\n",
    "#   P0 - initial state probabilities\n",
    "#   mu - observation mean\n",
    "#   sigma - observation variance\n",
    "#   scale - see 'ghmm_fwd'\n",
    "\n",
    "# Output:\n",
    "#   beta - Pr( s_t, observation sequence from time 't+1')\n",
    "\n",
    "    scale = np.exp(scale)\n",
    "    N = np.shape(A)[0];\n",
    "    T = np.shape(Y)[0]\n",
    "    M = np.shape(Y)[1]\n",
    "    Pys = np.zeros([N,T])\n",
    "    \n",
    "    for j in range(N):\n",
    "        sigmai = np.reshape(sigma[j,:,:],[M,M],order='F')\n",
    "        for t in range(T):\n",
    "            Pys[j,t] = np.sqrt(1/(((2*np.pi)**M)*np.linalg.det(sigmai)))*np.exp(np.dot(-0.5*(Y[t,:]-mu[j,:]),np.dot(np.linalg.inv(sigmai),(Y[t,:]-mu[j,:]).reshape(-1,1))))\n",
    "            \n",
    "    beta = np.zeros([N,T])\n",
    "    beta[:,T-1] = (np.ones([N,1])/scale[T-1]).flatten()\n",
    "    for t in range(T-2,-1,-1):\n",
    "        # Hint: Use np.reshape(-1,1) to make a 2-d vector from a 1-d vector. \n",
    "        #   This changes how numpy handles certain operations such as '*'.\n",
    "        beta[:,t] = ???\n",
    "    beta = np.log(beta+1.0e-10)\n",
    "    \n",
    "    return beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "Now that you can learn a HMM, you can use the learned parameters with ghmm_fwd to predict the likelihood of a sequence belonging to that HMM. \n",
    "\n",
    "You now have to evaluate how well the HMMs can distinguish between the utterances of the digit \"2\" and of the digit \"5\" by using the \"leave one out\" scheme. \n",
    "\n",
    "There are a total of 20 utterances. Learn a HMM for the digit \"2\" and another for the digit \"5\" using 19 utterances of the data and test it on the single utterance that you have left out. Repeat this 20 times until you have tested all 20 utterances. \n",
    "\n",
    "Do this procedure for:\n",
    "* Only the audio features\n",
    "* Only the video features\n",
    "* The audio and video features combined (concatenated).\n",
    "\n",
    "Determine the performance of each of these sets of features by calculating the accuracy: $\\frac{\\#correct}{20}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"AV_DATA\"\n",
    "N = 5\n",
    "Ainit = np.array([[0.8,0.2,0,0,0],[0,0.8,0.2,0,0],[0,0,0.8,0.2,0],[0,0,0,0.8,0.2],[0,0,0,0,1]])\n",
    "\n",
    "# Enter your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
